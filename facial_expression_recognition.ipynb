{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Facial Expression Recognition with Multi-Task Learning\n",
        "\n",
        "This notebook implements a comprehensive facial expression recognition system using PyTorch with multi-task learning for:\n",
        "- Expression classification (8 classes)\n",
        "- Valence regression\n",
        "- Arousal regression\n",
        "\n",
        "## Features:\n",
        "- Multiple model architectures (ResNet18, MobileNetV2, UltraSimple)\n",
        "- Data augmentation and preprocessing\n",
        "- Comprehensive evaluation metrics\n",
        "- Visualization tools\n",
        "- GPU optimization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Imports and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('Agg')\n",
        "plt.ioff()\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "from typing import Tuple, List, Dict, Any\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader as TorchDataLoader\n",
        "from torchvision import transforms, models\n",
        "from sklearn.metrics import (accuracy_score, f1_score, cohen_kappa_score, \n",
        "                           roc_auc_score, average_precision_score, \n",
        "                           mean_squared_error, classification_report,\n",
        "                           confusion_matrix)\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from scipy.stats import pearsonr\n",
        "from scipy import stats\n",
        "import krippendorff\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU detected: NVIDIA GeForce MX230\n",
            "GPU memory: 2.1 GB\n",
            "‚úÖ GPU optimizations enabled for maximum performance\n"
          ]
        }
      ],
      "source": [
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# Backward-compat alias for notebooks/snippets that expect `DEVICE`\n",
        "DEVICE = device\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU detected: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    # Enable memory optimization\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.backends.cudnn.deterministic = False  # Allow non-deterministic for speed\n",
        "    # Enable memory efficient attention if available\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "    # Prefer high matmul precision on Ampere+\n",
        "    try:\n",
        "        torch.set_float32_matmul_precision('high')\n",
        "    except Exception:\n",
        "        pass\n",
        "    # Clear cache\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"‚úÖ GPU optimizations enabled for maximum performance\")\n",
        "else:\n",
        "    print(\"GPU not available, using CPU\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Config:\n",
        "    \"\"\"Configuration class for hyperparameters and paths\"\"\"\n",
        "    \n",
        "    # Paths\n",
        "    DATASET_PATH = \"Dataset\"\n",
        "    IMAGES_PATH = os.path.join(DATASET_PATH, \"images\")\n",
        "    ANNOTATIONS_PATH = os.path.join(DATASET_PATH, \"annotations\")\n",
        "    \n",
        "    # Model parameters - Ultra conservative for stability\n",
        "    IMG_SIZE = 224\n",
        "    BATCH_SIZE = 16  # Very small batch size\n",
        "    EPOCHS = 10  # run for two epochs\n",
        "    LEARNING_RATE = 1e-5  # Extremely conservative learning rate\n",
        "    # Single-model selection for notebook conversion\n",
        "    # Options: 'UltraSimple', 'ResNet18', 'MobileNetV2'\n",
        "    MODEL_NAME = 'ResNet18'\n",
        "    # Whether to include regression losses during training\n",
        "    USE_REGRESSION_LOSSES = False\n",
        "    \n",
        "    # Classes\n",
        "    NUM_CLASSES = 8\n",
        "    CLASS_NAMES = ['Neutral', 'Happy', 'Sad', 'Surprise', 'Fear', 'Disgust', 'Anger', 'Contempt']\n",
        "    \n",
        "    # Multi-task loss weights\n",
        "    CLASSIFICATION_WEIGHT = 1.0\n",
        "    VALENCE_WEIGHT = 0.5\n",
        "    AROUSAL_WEIGHT = 0.5\n",
        "    \n",
        "    # Data split\n",
        "    TRAIN_SPLIT = 0.8\n",
        "    VAL_SPLIT = 0.2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Loading and Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FacialExpressionDataLoader:\n",
        "    \"\"\"Data loading and preprocessing class following Keras Idiomatic Programmer patterns\"\"\"\n",
        "    \n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "        self.image_files = []\n",
        "        self.annotations = {}\n",
        "        \n",
        "    def load_dataset(self) -> Tuple[np.ndarray, Dict[str, np.ndarray]]:\n",
        "        \"\"\"\n",
        "        Load and preprocess the complete dataset\n",
        "        \n",
        "        Returns:\n",
        "            images: Array of preprocessed images\n",
        "            labels: Dictionary containing expression, valence, arousal, landmarks\n",
        "        \"\"\"\n",
        "        print(\"Loading dataset...\")\n",
        "        \n",
        "        # Get all image files\n",
        "        image_files = [f for f in os.listdir(self.config.IMAGES_PATH) if f.endswith('.jpg')]\n",
        "        image_ids = [f.split('.')[0] for f in image_files]\n",
        "        \n",
        "        images = []\n",
        "        expressions = []\n",
        "        valences = []\n",
        "        arousals = []\n",
        "        landmarks = []\n",
        "        valid_indices = []\n",
        "        \n",
        "        for idx, image_id in enumerate(image_ids):\n",
        "            try:\n",
        "                # Load image\n",
        "                img_path = os.path.join(self.config.IMAGES_PATH, f\"{image_id}.jpg\")\n",
        "                img = cv2.imread(img_path)\n",
        "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "                img = cv2.resize(img, (self.config.IMG_SIZE, self.config.IMG_SIZE))\n",
        "                \n",
        "                # Load annotations\n",
        "                exp = np.load(os.path.join(self.config.ANNOTATIONS_PATH, f\"{image_id}_exp.npy\"))\n",
        "                val = np.load(os.path.join(self.config.ANNOTATIONS_PATH, f\"{image_id}_val.npy\"))\n",
        "                aro = np.load(os.path.join(self.config.ANNOTATIONS_PATH, f\"{image_id}_aro.npy\"))\n",
        "                lnd = np.load(os.path.join(self.config.ANNOTATIONS_PATH, f\"{image_id}_lnd.npy\"))\n",
        "                \n",
        "                # Convert to proper numeric types\n",
        "                exp = float(exp)\n",
        "                val = float(val)\n",
        "                aro = float(aro)\n",
        "                lnd = lnd.astype(np.float32)\n",
        "                \n",
        "                # Filter out invalid entries (valence or arousal == -2)\n",
        "                if val != -2 and aro != -2:\n",
        "                    images.append(img)\n",
        "                    expressions.append(int(exp))\n",
        "                    valences.append(float(val))\n",
        "                    arousals.append(float(aro))\n",
        "                    landmarks.append(lnd)\n",
        "                    valid_indices.append(idx)\n",
        "                    \n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {image_id}: {e}\")\n",
        "                continue\n",
        "        \n",
        "        print(f\"Loaded {len(images)} valid samples from {len(image_ids)} total images\")\n",
        "        \n",
        "        # Convert to numpy arrays\n",
        "        images = np.array(images, dtype=np.float32) / 255.0  # Normalize to [0,1]\n",
        "        # Ensure images are in [H, W, C] format\n",
        "        if len(images.shape) == 4 and images.shape[-1] == 3:\n",
        "            pass  # Already in correct format\n",
        "        else:\n",
        "            print(f\"Warning: Unexpected image shape: {images.shape}\")\n",
        "        \n",
        "        expressions = np.array(expressions)\n",
        "        valences = np.array(valences)\n",
        "        arousals = np.array(arousals)\n",
        "        landmarks = np.array(landmarks)\n",
        "        \n",
        "        # Prepare labels dictionary\n",
        "        labels = {\n",
        "            'expression': expressions,\n",
        "            'valence': valences,\n",
        "            'arousal': arousals,\n",
        "            'landmarks': landmarks\n",
        "        }\n",
        "        \n",
        "        return images, labels\n",
        "    \n",
        "    def create_data_splits(self, images: np.ndarray, labels: Dict[str, np.ndarray]) -> Tuple:\n",
        "        \"\"\"\n",
        "        Create train/validation splits\n",
        "        \n",
        "        Args:\n",
        "            images: Image data\n",
        "            labels: Label dictionary\n",
        "            \n",
        "        Returns:\n",
        "            Tuple of (train_images, train_labels, val_images, val_labels)\n",
        "        \"\"\"\n",
        "        n_samples = len(images)\n",
        "        n_train = int(n_samples * self.config.TRAIN_SPLIT)\n",
        "        \n",
        "        # Random shuffle\n",
        "        indices = np.random.permutation(n_samples)\n",
        "        train_indices = indices[:n_train]\n",
        "        val_indices = indices[n_train:]\n",
        "        \n",
        "        # Split data\n",
        "        train_images = images[train_indices]\n",
        "        val_images = images[val_indices]\n",
        "        \n",
        "        train_labels = {key: val[train_indices] for key, val in labels.items()}\n",
        "        val_labels = {key: val[val_indices] for key, val in labels.items()}\n",
        "        \n",
        "        print(f\"Training samples: {len(train_images)}\")\n",
        "        print(f\"Validation samples: {len(val_images)}\")\n",
        "        \n",
        "        return train_images, train_labels, val_images, val_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FacialExpressionDataset(Dataset):\n",
        "    \"\"\"PyTorch Dataset for facial expression recognition\"\"\"\n",
        "    \n",
        "    def __init__(self, images: np.ndarray, labels: Dict[str, np.ndarray], \n",
        "                 config: Config, transform=None, is_training=True):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.config = config\n",
        "        self.transform = transform\n",
        "        self.is_training = is_training\n",
        "        \n",
        "        # Convert to tensors\n",
        "        self.expressions = torch.LongTensor(labels['expression'])\n",
        "        self.valences = torch.FloatTensor(labels['valence'])\n",
        "        self.arousals = torch.FloatTensor(labels['arousal'])\n",
        "        # Normalize landmarks to [0,1] using image size (assumed 0..IMG_SIZE)\n",
        "        lnd = labels['landmarks']\n",
        "        # If landmarks are pixel coords, scale by IMG_SIZE; guard divide-by-zero\n",
        "        denom = max(1, self.config.IMG_SIZE)\n",
        "        self.landmarks = torch.FloatTensor(lnd / denom)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # Get image\n",
        "        image = self.images[idx]\n",
        "        \n",
        "        # Convert to PIL Image for transforms\n",
        "        if isinstance(image, np.ndarray):\n",
        "            image = (image * 255).astype(np.uint8)\n",
        "            # Keep as ndarray; transforms pipeline will handle ToPILImage\n",
        "        \n",
        "        # Apply transforms\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        else:\n",
        "            # Ensure proper tensor format: [C, H, W]\n",
        "            image = transforms.ToTensor()(image)\n",
        "        \n",
        "        # Get labels\n",
        "        expression = self.expressions[idx]\n",
        "        valence = self.valences[idx]\n",
        "        arousal = self.arousals[idx]\n",
        "        landmarks = self.landmarks[idx]\n",
        "        \n",
        "        return {\n",
        "            'image': image,\n",
        "            'landmarks': landmarks,\n",
        "            'expression': expression,\n",
        "            'valence': valence,\n",
        "            'arousal': arousal\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DataAugmentation:\n",
        "    \"\"\"Data augmentation class for training data\"\"\"\n",
        "    \n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "        \n",
        "        # Training transforms with augmentation\n",
        "        self.train_transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.RandomRotation(30),\n",
        "            transforms.RandomResizedCrop(config.IMG_SIZE, scale=(0.8, 1.0)),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        \n",
        "        # Validation transforms without augmentation\n",
        "        self.val_transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize((config.IMG_SIZE, config.IMG_SIZE)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "    \n",
        "    def get_transforms(self, is_training=True):\n",
        "        \"\"\"Get appropriate transforms\"\"\"\n",
        "        return self.train_transform if is_training else self.val_transform\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Definitions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "class UltraSimpleModel(nn.Module):\n",
        "    \"\"\"Ultra simple model for debugging NaN issues\"\"\"\n",
        "    \n",
        "    def __init__(self, config: Config):\n",
        "        super(UltraSimpleModel, self).__init__()\n",
        "        self.config = config\n",
        "        \n",
        "        # Very simple CNN\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            \n",
        "            nn.Conv2d(16, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            \n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((4, 4))\n",
        "        )\n",
        "        \n",
        "        # Simple landmark processing\n",
        "        self.landmark_net = nn.Sequential(\n",
        "            nn.Linear(136, 32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        # Simple fusion\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Linear(64 * 4 * 4 + 32, 128),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        # Only expression classification for now\n",
        "        self.expression_head = nn.Linear(128, config.NUM_CLASSES)\n",
        "        \n",
        "        # Initialize weights very conservatively\n",
        "        self._initialize_weights()\n",
        "    \n",
        "    def _initialize_weights(self):\n",
        "        \"\"\"Very conservative weight initialization\"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, mean=0, std=0.01)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Conv2d):\n",
        "                nn.init.normal_(m.weight, mean=0, std=0.01)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "    \n",
        "    def forward(self, image, landmarks):\n",
        "        # Extract image features\n",
        "        img_features = self.conv_layers(image)\n",
        "        img_features = img_features.view(img_features.size(0), -1)\n",
        "        \n",
        "        # Process landmarks\n",
        "        landmark_features = self.landmark_net(landmarks)\n",
        "        \n",
        "        # Fuse features\n",
        "        fused_features = torch.cat([img_features, landmark_features], dim=1)\n",
        "        fused_features = self.fusion(fused_features)\n",
        "        \n",
        "        # Only expression output for now\n",
        "        expression_output = self.expression_head(fused_features)\n",
        "        \n",
        "        # Return dummy outputs for valence/arousal to maintain compatibility\n",
        "        batch_size = expression_output.size(0)\n",
        "        valence_output = torch.zeros(batch_size, 1, device=expression_output.device)\n",
        "        arousal_output = torch.zeros(batch_size, 1, device=expression_output.device)\n",
        "        \n",
        "        return {\n",
        "            'expression': expression_output,\n",
        "            'valence': valence_output,\n",
        "            'arousal': arousal_output\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiTaskNet(nn.Module):\n",
        "    \"\"\"Unified multi-task head wrapper for CNN backbones (expression, valence, arousal).\"\"\"\n",
        "    def __init__(self, backbone: nn.Module, feature_dim: int, num_classes: int):\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        self.expression_head = nn.Linear(feature_dim, num_classes)\n",
        "        self.arousal_head = nn.Linear(feature_dim, 1)\n",
        "        self.valence_head = nn.Linear(feature_dim, 1)\n",
        "\n",
        "        # Light-weight initialization for heads\n",
        "        nn.init.xavier_uniform_(self.expression_head.weight, gain=0.1)\n",
        "        nn.init.constant_(self.expression_head.bias, 0)\n",
        "        nn.init.xavier_uniform_(self.arousal_head.weight, gain=0.1)\n",
        "        nn.init.constant_(self.arousal_head.bias, 0)\n",
        "        nn.init.xavier_uniform_(self.valence_head.weight, gain=0.1)\n",
        "        nn.init.constant_(self.valence_head.bias, 0)\n",
        "\n",
        "    def forward(self, image, landmarks=None):\n",
        "        # Backbones here produce flat feature vectors already (Identity classifiers)\n",
        "        features = self.backbone(image)\n",
        "        # Some backbones may return tuples\n",
        "        if isinstance(features, (list, tuple)):\n",
        "            features = features[0]\n",
        "        expression_output = self.expression_head(features)\n",
        "        # Keep regression heads unconstrained; evaluation can clip if needed\n",
        "        arousal_output = self.arousal_head(features)\n",
        "        valence_output = self.valence_head(features)\n",
        "        return {\n",
        "            'expression': expression_output,\n",
        "            'valence': valence_output,\n",
        "            'arousal': arousal_output\n",
        "        }\n",
        "\n",
        "def build_resnet18_multitask(num_classes: int) -> nn.Module:\n",
        "    \"\"\"ResNet18 backbone with multi-task heads (pretrained on ImageNet).\"\"\"\n",
        "    m = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "    feat_dim = m.fc.in_features\n",
        "    m.fc = nn.Identity()\n",
        "    return MultiTaskNet(m, feat_dim, num_classes)\n",
        "\n",
        "def build_mobilenet_v2_multitask(num_classes: int) -> nn.Module:\n",
        "    \"\"\"MobileNetV2 backbone with multi-task heads (pretrained on ImageNet).\"\"\"\n",
        "    m = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)\n",
        "    feat_dim = m.last_channel\n",
        "    # Expose flat pooled features by bypassing classifier\n",
        "    m.classifier = nn.Identity()\n",
        "    return MultiTaskNet(m, feat_dim, num_classes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Metrics and Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MetricsCalculator:\n",
        "    \"\"\"Comprehensive metrics calculation for multi-task learning\"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def classification_metrics(y_true: np.ndarray, y_pred: np.ndarray, \n",
        "                             y_pred_proba: np.ndarray = None) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Calculate classification metrics\n",
        "        \n",
        "        Args:\n",
        "            y_true: Ground truth labels\n",
        "            y_pred: Predicted labels\n",
        "            y_pred_proba: Predicted probabilities\n",
        "            \n",
        "        Returns:\n",
        "            Dictionary of metrics\n",
        "        \"\"\"\n",
        "        metrics = {}\n",
        "        \n",
        "        # Basic metrics\n",
        "        metrics['accuracy'] = accuracy_score(y_true, y_pred)\n",
        "        metrics['f1_macro'] = f1_score(y_true, y_pred, average='macro')\n",
        "        metrics['f1_weighted'] = f1_score(y_true, y_pred, average='weighted')\n",
        "        metrics['cohen_kappa'] = cohen_kappa_score(y_true, y_pred)\n",
        "        \n",
        "        # Krippendorff's Alpha (requires specific format)\n",
        "        try:\n",
        "            # Convert to reliability data format for krippendorff\n",
        "            reliability_data = np.array([y_true, y_pred])\n",
        "            metrics['krippendorff_alpha'] = krippendorff.alpha(reliability_data, level_of_measurement='ordinal')\n",
        "        except:\n",
        "            metrics['krippendorff_alpha'] = 0.0\n",
        "        \n",
        "        # Multi-class AUC metrics\n",
        "        if y_pred_proba is not None:\n",
        "            try:\n",
        "                # One-vs-Rest AUC\n",
        "                y_true_bin = label_binarize(y_true, classes=range(8))\n",
        "                metrics['auc_roc_ovr'] = roc_auc_score(y_true_bin, y_pred_proba, \n",
        "                                                      multi_class='ovr', average='macro')\n",
        "                metrics['auc_pr_macro'] = average_precision_score(y_true_bin, y_pred_proba, \n",
        "                                                                 average='macro')\n",
        "            except:\n",
        "                metrics['auc_roc_ovr'] = 0.0\n",
        "                metrics['auc_pr_macro'] = 0.0\n",
        "        \n",
        "        return metrics\n",
        "    \n",
        "    @staticmethod\n",
        "    def regression_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Calculate regression metrics\n",
        "        \n",
        "        Args:\n",
        "            y_true: Ground truth values\n",
        "            y_pred: Predicted values\n",
        "            \n",
        "        Returns:\n",
        "            Dictionary of metrics\n",
        "        \"\"\"\n",
        "        metrics = {}\n",
        "        \n",
        "        # RMSE\n",
        "        metrics['rmse'] = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "        \n",
        "        # Pearson Correlation\n",
        "        corr, p_value = pearsonr(y_true.flatten(), y_pred.flatten())\n",
        "        metrics['pearson_corr'] = corr\n",
        "        metrics['pearson_p_value'] = p_value\n",
        "        \n",
        "        # Sign Agreement Metric (SAGR)\n",
        "        # Penalize sign mismatches and deviation\n",
        "        sign_true = np.sign(y_true)\n",
        "        sign_pred = np.sign(y_pred)\n",
        "        sign_agreement = np.mean(sign_true == sign_pred)\n",
        "        \n",
        "        # Additional penalty for magnitude deviation\n",
        "        magnitude_penalty = np.mean(np.abs(y_true - y_pred))\n",
        "        metrics['sagr'] = sign_agreement - 0.1 * magnitude_penalty\n",
        "        \n",
        "        # Concordance Correlation Coefficient (CCC)\n",
        "        # CCC = 2 * rho * std_x * std_y / (std_x^2 + std_y^2 + (mean_x - mean_y)^2)\n",
        "        mean_true = np.mean(y_true)\n",
        "        mean_pred = np.mean(y_pred)\n",
        "        std_true = np.std(y_true)\n",
        "        std_pred = np.std(y_pred)\n",
        "        \n",
        "        numerator = 2 * corr * std_true * std_pred\n",
        "        denominator = std_true**2 + std_pred**2 + (mean_true - mean_pred)**2\n",
        "        metrics['ccc'] = numerator / (denominator + 1e-8)  # Add small epsilon to avoid division by zero\n",
        "        \n",
        "        return metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Training Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    \"\"\"Training pipeline manager\"\"\"\n",
        "    \n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "        self.history = {}\n",
        "        self.device = device\n",
        "        \n",
        "    def create_data_loaders(self, train_data: Tuple, val_data: Tuple, augmentation: DataAugmentation):\n",
        "        \"\"\"Create PyTorch data loaders\"\"\"\n",
        "        train_images, train_labels = train_data\n",
        "        val_images, val_labels = val_data\n",
        "        \n",
        "        # Create datasets\n",
        "        train_dataset = FacialExpressionDataset(\n",
        "            train_images, train_labels, self.config, \n",
        "            transform=augmentation.get_transforms(is_training=True), \n",
        "            is_training=True\n",
        "        )\n",
        "        \n",
        "        val_dataset = FacialExpressionDataset(\n",
        "            val_images, val_labels, self.config,\n",
        "            transform=augmentation.get_transforms(is_training=False),\n",
        "            is_training=False\n",
        "        )\n",
        "        \n",
        "        # Create data loaders with Windows-compatible parameters\n",
        "        train_loader = TorchDataLoader(\n",
        "            train_dataset, \n",
        "            batch_size=self.config.BATCH_SIZE, \n",
        "            shuffle=True, \n",
        "            num_workers=0,  # Set to 0 for Windows compatibility\n",
        "            pin_memory=True,\n",
        "        )\n",
        "        \n",
        "        val_loader = TorchDataLoader(\n",
        "            val_dataset, \n",
        "            batch_size=self.config.BATCH_SIZE, \n",
        "            shuffle=False, \n",
        "            num_workers=0,  # Set to 0 for Windows compatibility\n",
        "            pin_memory=True,\n",
        "        )\n",
        "        \n",
        "        return train_loader, val_loader\n",
        "    \n",
        "    def compute_loss(self, outputs: Dict, targets: Dict):\n",
        "        \"\"\"Compute loss - focus only on expression classification for debugging\"\"\"\n",
        "        # Classification loss (with label smoothing to avoid collapse)\n",
        "        exp_loss = F.cross_entropy(\n",
        "            outputs['expression'], targets['expression'], label_smoothing=0.1\n",
        "        )\n",
        "\n",
        "        if getattr(self.config, 'USE_REGRESSION_LOSSES', False):\n",
        "            # Optional regression losses when heads are present\n",
        "            val_loss = F.mse_loss(outputs['valence'].squeeze(), targets['valence'])\n",
        "            aro_loss = F.mse_loss(outputs['arousal'].squeeze(), targets['arousal'])\n",
        "            total_loss = (\n",
        "                self.config.CLASSIFICATION_WEIGHT * exp_loss\n",
        "                + self.config.VALENCE_WEIGHT * val_loss\n",
        "                + self.config.AROUSAL_WEIGHT * aro_loss\n",
        "            )\n",
        "        else:\n",
        "            # Dummy losses for compatibility\n",
        "            val_loss = torch.tensor(0.0, device=exp_loss.device)\n",
        "            aro_loss = torch.tensor(0.0, device=exp_loss.device)\n",
        "            total_loss = exp_loss\n",
        "        \n",
        "        return {\n",
        "            'total_loss': total_loss,\n",
        "            'expression_loss': exp_loss,\n",
        "            'valence_loss': val_loss,\n",
        "            'arousal_loss': aro_loss\n",
        "        }\n",
        "    \n",
        "    def train_model(self, model: nn.Module, train_data: Tuple, val_data: Tuple, \n",
        "                   model_name: str, augmentation: DataAugmentation) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Train a PyTorch model with the given data\n",
        "        \n",
        "        Args:\n",
        "            model: PyTorch model to train\n",
        "            train_data: Training data tuple (images, labels)\n",
        "            val_data: Validation data tuple (images, labels)\n",
        "            model_name: Name for saving/logging\n",
        "            augmentation: Data augmentation object\n",
        "            \n",
        "        Returns:\n",
        "            Training history and metrics\n",
        "        \"\"\"\n",
        "        # Move model to device\n",
        "        model = model.to(self.device)\n",
        "        \n",
        "        # Create data loaders\n",
        "        train_loader, val_loader = self.create_data_loaders(train_data, val_data, augmentation)\n",
        "        \n",
        "        # Setup optimizer with optimized parameters\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=self.config.LEARNING_RATE, weight_decay=1e-4, eps=1e-8)\n",
        "        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)\n",
        "        \n",
        "        # Training history\n",
        "        history = {\n",
        "            'train_loss': [], 'val_loss': [],\n",
        "            'train_exp_acc': [], 'val_exp_acc': [],\n",
        "            'train_val_mse': [], 'val_val_mse': [],\n",
        "            'train_aro_mse': [], 'val_aro_mse': []\n",
        "        }\n",
        "        \n",
        "        print(f\"\\nTraining {model_name}...\")\n",
        "        print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "        \n",
        "        best_val_loss = float('inf')\n",
        "        patience_counter = 0\n",
        "        \n",
        "        import time\n",
        "        start_time = time.time()\n",
        "        \n",
        "        # Enable mixed precision training for faster training\n",
        "        scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n",
        "        \n",
        "        for epoch in range(self.config.EPOCHS):\n",
        "            epoch_start_time = time.time()\n",
        "            \n",
        "            # Training phase\n",
        "            model.train()\n",
        "            train_losses = []\n",
        "            train_exp_correct = 0\n",
        "            train_total = 0\n",
        "            train_val_mse = 0\n",
        "            train_aro_mse = 0\n",
        "            \n",
        "            # Progress tracking\n",
        "            num_batches = len(train_loader)\n",
        "            \n",
        "            for batch_idx, batch in enumerate(train_loader):\n",
        "                # Move data to device (non-blocking for better performance)\n",
        "                images = batch['image'].to(self.device, non_blocking=True)\n",
        "                # Use channels_last for better GPU throughput on NVIDIA\n",
        "                if images.is_cuda:\n",
        "                    images = images.to(memory_format=torch.channels_last)\n",
        "                landmarks = batch['landmarks'].to(self.device, non_blocking=True)\n",
        "                exp_targets = batch['expression'].to(self.device, non_blocking=True)\n",
        "                val_targets = batch['valence'].to(self.device, non_blocking=True)\n",
        "                aro_targets = batch['arousal'].to(self.device, non_blocking=True)\n",
        "                \n",
        "                # Debug: Check input values\n",
        "                if batch_idx == 0:\n",
        "                    print(f\"üîç Debug - Batch {batch_idx+1}:\")\n",
        "                    print(f\"  Images: min={images.min():.4f}, max={images.max():.4f}, mean={images.mean():.4f}\")\n",
        "                    print(f\"  Landmarks: min={landmarks.min():.4f}, max={landmarks.max():.4f}, mean={landmarks.mean():.4f}\")\n",
        "                    print(f\"  Expression targets: {exp_targets[:5]}\")\n",
        "                    print(f\"  Valence targets: {val_targets[:5]}\")\n",
        "                    print(f\"  Arousal targets: {aro_targets[:5]}\")\n",
        "                \n",
        "                # Mixed precision training\n",
        "                if scaler is not None:\n",
        "                    with torch.cuda.amp.autocast():\n",
        "                        outputs = model(images, landmarks)\n",
        "                        targets = {\n",
        "                            'expression': exp_targets,\n",
        "                            'valence': val_targets,\n",
        "                            'arousal': aro_targets\n",
        "                        }\n",
        "                        losses = self.compute_loss(outputs, targets)\n",
        "                    \n",
        "                    # Check for NaN loss\n",
        "                    if torch.isnan(losses['total_loss']):\n",
        "                        print(f\"‚ö†Ô∏è  NaN loss detected at batch {batch_idx+1}, skipping...\")\n",
        "                        if batch_idx < 5:  # Debug first few batches\n",
        "                            print(f\"  Outputs: exp={outputs['expression'][:2]}, val={outputs['valence'][:2]}, aro={outputs['arousal'][:2]}\")\n",
        "                        continue\n",
        "                    \n",
        "                    optimizer.zero_grad()\n",
        "                    scaler.scale(losses['total_loss']).backward()\n",
        "                    # Gradient clipping\n",
        "                    scaler.unscale_(optimizer)\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "                else:\n",
        "                    outputs = model(images, landmarks)\n",
        "                    targets = {\n",
        "                        'expression': exp_targets,\n",
        "                        'valence': val_targets,\n",
        "                        'arousal': aro_targets\n",
        "                    }\n",
        "                    losses = self.compute_loss(outputs, targets)\n",
        "                    \n",
        "                    # Check for NaN loss\n",
        "                    if torch.isnan(losses['total_loss']):\n",
        "                        print(f\"‚ö†Ô∏è  NaN loss detected at batch {batch_idx+1}, skipping...\")\n",
        "                        continue\n",
        "                    \n",
        "                    optimizer.zero_grad()\n",
        "                    losses['total_loss'].backward()\n",
        "                    # Gradient clipping\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                    optimizer.step()\n",
        "                \n",
        "                # Track metrics\n",
        "                train_losses.append(losses['total_loss'].item())\n",
        "                \n",
        "                # Expression accuracy\n",
        "                exp_pred = torch.argmax(outputs['expression'], dim=1)\n",
        "                train_exp_correct += (exp_pred == exp_targets).sum().item()\n",
        "                train_total += exp_targets.size(0)\n",
        "                \n",
        "                # Regression MSE\n",
        "                train_val_mse += F.mse_loss(outputs['valence'].squeeze(), val_targets).item()\n",
        "                train_aro_mse += F.mse_loss(outputs['arousal'].squeeze(), aro_targets).item()\n",
        "                \n",
        "                # Print progress every 10% of batches\n",
        "                if (batch_idx + 1) % max(1, num_batches // 10) == 0:\n",
        "                    progress = (batch_idx + 1) / num_batches * 100\n",
        "                    print(f\"  Batch {batch_idx+1}/{num_batches} ({progress:.1f}%) - Loss: {losses['total_loss'].item():.4f}\")\n",
        "            \n",
        "            # Validation phase\n",
        "            model.eval()\n",
        "            val_losses = []\n",
        "            val_exp_correct = 0\n",
        "            val_total = 0\n",
        "            val_val_mse = 0\n",
        "            val_aro_mse = 0\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                for batch in val_loader:\n",
        "                    images = batch['image'].to(self.device, non_blocking=True)\n",
        "                    if images.is_cuda:\n",
        "                        images = images.to(memory_format=torch.channels_last)\n",
        "                    landmarks = batch['landmarks'].to(self.device, non_blocking=True)\n",
        "                    exp_targets = batch['expression'].to(self.device, non_blocking=True)\n",
        "                    val_targets = batch['valence'].to(self.device, non_blocking=True)\n",
        "                    aro_targets = batch['arousal'].to(self.device, non_blocking=True)\n",
        "                    \n",
        "                    if scaler is not None:\n",
        "                        with torch.cuda.amp.autocast():\n",
        "                            outputs = model(images, landmarks)\n",
        "                            targets = {\n",
        "                                'expression': exp_targets,\n",
        "                                'valence': val_targets,\n",
        "                                'arousal': aro_targets\n",
        "                            }\n",
        "                            losses = self.compute_loss(outputs, targets)\n",
        "                    else:\n",
        "                        outputs = model(images, landmarks)\n",
        "                        targets = {\n",
        "                            'expression': exp_targets,\n",
        "                            'valence': val_targets,\n",
        "                            'arousal': aro_targets\n",
        "                        }\n",
        "                        losses = self.compute_loss(outputs, targets)\n",
        "                    \n",
        "                    val_losses.append(losses['total_loss'].item())\n",
        "                    \n",
        "                    # Expression accuracy\n",
        "                    exp_pred = torch.argmax(outputs['expression'], dim=1)\n",
        "                    val_exp_correct += (exp_pred == exp_targets).sum().item()\n",
        "                    val_total += exp_targets.size(0)\n",
        "                    \n",
        "                    # Regression MSE\n",
        "                    val_val_mse += F.mse_loss(outputs['valence'].squeeze(), val_targets).item()\n",
        "                    val_aro_mse += F.mse_loss(outputs['arousal'].squeeze(), aro_targets).item()\n",
        "            \n",
        "            # Calculate epoch metrics\n",
        "            train_loss = np.mean(train_losses)\n",
        "            val_loss = np.mean(val_losses)\n",
        "            train_exp_acc = train_exp_correct / train_total\n",
        "            val_exp_acc = val_exp_correct / val_total\n",
        "            train_val_mse_avg = train_val_mse / len(train_loader)\n",
        "            val_val_mse_avg = val_val_mse / len(val_loader)\n",
        "            train_aro_mse_avg = train_aro_mse / len(train_loader)\n",
        "            val_aro_mse_avg = val_aro_mse / len(val_loader)\n",
        "            \n",
        "            # Update history\n",
        "            history['train_loss'].append(train_loss)\n",
        "            history['val_loss'].append(val_loss)\n",
        "            history['train_exp_acc'].append(train_exp_acc)\n",
        "            history['val_exp_acc'].append(val_exp_acc)\n",
        "            history['train_val_mse'].append(train_val_mse_avg)\n",
        "            history['val_val_mse'].append(val_val_mse_avg)\n",
        "            history['train_aro_mse'].append(train_aro_mse_avg)\n",
        "            history['val_aro_mse'].append(val_aro_mse_avg)\n",
        "            \n",
        "            # Calculate epoch time\n",
        "            epoch_time = time.time() - epoch_start_time\n",
        "            \n",
        "            # Print detailed epoch results\n",
        "            print(f'\\n{\"=\"*80}')\n",
        "            print(f'EPOCH {epoch+1}/{self.config.EPOCHS} - {model_name}')\n",
        "            print(f'{\"=\"*80}')\n",
        "            print(f'‚è±Ô∏è  Epoch Time: {epoch_time:.2f}s')\n",
        "            print(f'üìä Training Metrics:')\n",
        "            print(f'   Loss: {train_loss:.4f} | Accuracy: {train_exp_acc:.4f}')\n",
        "            print(f'   Valence MSE: {train_val_mse_avg:.4f} | Arousal MSE: {train_aro_mse_avg:.4f}')\n",
        "            print(f'üìà Validation Metrics:')\n",
        "            print(f'   Loss: {val_loss:.4f} | Accuracy: {val_exp_acc:.4f}')\n",
        "            print(f'   Valence MSE: {val_val_mse_avg:.4f} | Arousal MSE: {val_aro_mse_avg:.4f}')\n",
        "            print(f'üéØ Learning Rate: {optimizer.param_groups[0][\"lr\"]:.2e}')\n",
        "            \n",
        "            # Learning rate scheduling\n",
        "            scheduler.step()\n",
        "            \n",
        "            # Early stopping\n",
        "            if np.isfinite(val_loss) and val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                patience_counter = 0\n",
        "                # Save best model\n",
        "                torch.save(model.state_dict(), f'best_{model_name}.pth')\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= 10:\n",
        "                    print(f'Early stopping at epoch {epoch+1}')\n",
        "                    break\n",
        "        \n",
        "        training_time = time.time() - start_time\n",
        "        \n",
        "        # Load best model if it exists; otherwise save current model as best\n",
        "        best_ckpt_path = f'best_{model_name}.pth'\n",
        "        if os.path.exists(best_ckpt_path):\n",
        "            model.load_state_dict(torch.load(best_ckpt_path))\n",
        "        else:\n",
        "            print(f\"\\n‚ö†Ô∏è  No best checkpoint found for {model_name}. Saving current model as best.\")\n",
        "            torch.save(model.state_dict(), best_ckpt_path)\n",
        "        \n",
        "        # Store results\n",
        "        self.history[model_name] = {\n",
        "            'history': history,\n",
        "            'training_time': training_time,\n",
        "            'model': model\n",
        "        }\n",
        "        \n",
        "        print(f\"\\n{model_name} training completed in {training_time:.2f} seconds\")\n",
        "        \n",
        "        return self.history[model_name]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This cell has been merged into the Trainer class above\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Evaluator:\n",
        "    \"\"\"Model evaluation and comparison\"\"\"\n",
        "    \n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "        self.results = {}\n",
        "        \n",
        "    def evaluate_model(self, model: nn.Module, test_data: Tuple, model_name: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Comprehensive evaluation of a trained model\n",
        "        \n",
        "        Args:\n",
        "            model: Trained PyTorch model\n",
        "            test_data: Test data tuple (images, labels)\n",
        "            model_name: Model name for logging\n",
        "            \n",
        "        Returns:\n",
        "            Comprehensive evaluation results\n",
        "        \"\"\"\n",
        "        test_images, test_labels = test_data\n",
        "        \n",
        "        print(f\"\\nEvaluating {model_name}...\")\n",
        "        \n",
        "        # Set model to evaluation mode\n",
        "        model.eval()\n",
        "        device = next(model.parameters()).device\n",
        "        \n",
        "        # Build evaluation dataset and dataloader to ensure correct tensor layout and normalization\n",
        "        val_transform = DataAugmentation(self.config).get_transforms(is_training=False)\n",
        "        eval_dataset = FacialExpressionDataset(\n",
        "            test_images, test_labels, self.config, transform=val_transform, is_training=False\n",
        "        )\n",
        "        eval_loader = TorchDataLoader(\n",
        "            eval_dataset,\n",
        "            batch_size=self.config.BATCH_SIZE,\n",
        "            shuffle=False,\n",
        "            num_workers=0,\n",
        "            pin_memory=True,\n",
        "        )\n",
        "\n",
        "        exp_pred_proba = []\n",
        "        val_pred = []\n",
        "        aro_pred = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in eval_loader:\n",
        "                batch_images = batch['image'].to(device, non_blocking=True)\n",
        "                if batch_images.is_cuda:\n",
        "                    batch_images = batch_images.to(memory_format=torch.channels_last)\n",
        "                batch_landmarks = batch['landmarks'].to(device, non_blocking=True)\n",
        "\n",
        "                outputs = model(batch_images, batch_landmarks)\n",
        "\n",
        "                exp_pred_proba.append(outputs['expression'].detach().cpu().numpy())\n",
        "                val_pred.append(outputs['valence'].detach().cpu().numpy().flatten())\n",
        "                aro_pred.append(outputs['arousal'].detach().cpu().numpy().flatten())\n",
        "        \n",
        "        # Concatenate all predictions\n",
        "        exp_pred_proba = np.concatenate(exp_pred_proba, axis=0)\n",
        "        val_pred = np.concatenate(val_pred, axis=0)\n",
        "        aro_pred = np.concatenate(aro_pred, axis=0)\n",
        "        \n",
        "        exp_pred = np.argmax(exp_pred_proba, axis=1)\n",
        "        \n",
        "        # Calculate metrics\n",
        "        # Classification metrics\n",
        "        exp_metrics = MetricsCalculator.classification_metrics(\n",
        "            test_labels['expression'], exp_pred, exp_pred_proba\n",
        "        )\n",
        "        \n",
        "        # Regression metrics\n",
        "        val_metrics = MetricsCalculator.regression_metrics(\n",
        "            test_labels['valence'], val_pred\n",
        "        )\n",
        "        \n",
        "        aro_metrics = MetricsCalculator.regression_metrics(\n",
        "            test_labels['arousal'], aro_pred\n",
        "        )\n",
        "        \n",
        "        # Combine results\n",
        "        results = {\n",
        "            'model_name': model_name,\n",
        "            'expression_metrics': exp_metrics,\n",
        "            'valence_metrics': val_metrics,\n",
        "            'arousal_metrics': aro_metrics,\n",
        "            'predictions': {\n",
        "                'expression': exp_pred,\n",
        "                'expression_proba': exp_pred_proba,\n",
        "                'valence': val_pred,\n",
        "                'arousal': aro_pred\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        self.results[model_name] = results\n",
        "        \n",
        "        # Print summary\n",
        "        print(f\"\\n{model_name} Results:\")\n",
        "        print(f\"Expression Accuracy: {exp_metrics['accuracy']:.4f}\")\n",
        "        print(f\"Expression F1 (macro): {exp_metrics['f1_macro']:.4f}\")\n",
        "        print(f\"Valence RMSE: {val_metrics['rmse']:.4f}\")\n",
        "        print(f\"Valence CCC: {val_metrics['ccc']:.4f}\")\n",
        "        print(f\"Arousal RMSE: {aro_metrics['rmse']:.4f}\")\n",
        "        print(f\"Arousal CCC: {aro_metrics['ccc']:.4f}\")\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def create_comparison_table(self) -> pd.DataFrame:\n",
        "        \"\"\"Create comparison table of all models\"\"\"\n",
        "        comparison_data = []\n",
        "        \n",
        "        for model_name, results in self.results.items():\n",
        "            row = {\n",
        "                'Model': model_name,\n",
        "                'Expression_Accuracy': results['expression_metrics']['accuracy'],\n",
        "                'Expression_F1': results['expression_metrics']['f1_macro'],\n",
        "                'Expression_Kappa': results['expression_metrics']['cohen_kappa'],\n",
        "                'Valence_RMSE': results['valence_metrics']['rmse'],\n",
        "                'Valence_CCC': results['valence_metrics']['ccc'],\n",
        "                'Valence_Corr': results['valence_metrics']['pearson_corr'],\n",
        "                'Arousal_RMSE': results['arousal_metrics']['rmse'],\n",
        "                'Arousal_CCC': results['arousal_metrics']['ccc'],\n",
        "                'Arousal_Corr': results['arousal_metrics']['pearson_corr']\n",
        "            }\n",
        "            comparison_data.append(row)\n",
        "        \n",
        "        return pd.DataFrame(comparison_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Visualizer:\n",
        "    \"\"\"Visualization utilities for results and analysis\"\"\"\n",
        "    \n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "        \n",
        "    def plot_training_history(self, history_dict: Dict[str, Any], save_path: str = None):\n",
        "        \"\"\"Plot training history for all models\"\"\"\n",
        "        plt.figure(figsize=(20, 12))\n",
        "        \n",
        "        metrics_to_plot = ['loss', 'expression_output_accuracy', 'valence_output_mse', 'arousal_output_mse']\n",
        "        \n",
        "        for i, metric in enumerate(metrics_to_plot, 1):\n",
        "            plt.subplot(2, 2, i)\n",
        "            \n",
        "            for model_name, model_data in history_dict.items():\n",
        "                history = model_data['history']\n",
        "                if metric in history:\n",
        "                    plt.plot(history[metric], label=f'{model_name} train')\n",
        "                    if f'val_{metric}' in history:\n",
        "                        plt.plot(history[f'val_{metric}'], label=f'{model_name} val', linestyle='--')\n",
        "            \n",
        "            plt.title(f'Training {metric.replace(\"_\", \" \").title()}')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel(metric.replace(\"_\", \" \").title())\n",
        "            plt.legend()\n",
        "            plt.grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        if save_path:\n",
        "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "    \n",
        "    def plot_confusion_matrix(self, y_true: np.ndarray, y_pred: np.ndarray, \n",
        "                            model_name: str, save_path: str = None):\n",
        "        \"\"\"Plot confusion matrix for expression classification\"\"\"\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        \n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                   xticklabels=self.config.CLASS_NAMES,\n",
        "                   yticklabels=self.config.CLASS_NAMES)\n",
        "        plt.title(f'Confusion Matrix - {model_name}')\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('Actual')\n",
        "        \n",
        "        if save_path:\n",
        "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "    \n",
        "    def plot_regression_scatter(self, y_true: np.ndarray, y_pred: np.ndarray, \n",
        "                              label: str, model_name: str, save_path: str = None):\n",
        "        \"\"\"Plot scatter plot for regression results\"\"\"\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.scatter(y_true, y_pred, alpha=0.6)\n",
        "        plt.plot([-1, 1], [-1, 1], 'r--', label='Perfect prediction')\n",
        "        plt.xlabel(f'True {label}')\n",
        "        plt.ylabel(f'Predicted {label}')\n",
        "        plt.title(f'{label} Prediction - {model_name}')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        \n",
        "        # Add correlation info\n",
        "        corr = np.corrcoef(y_true, y_pred)[0, 1]\n",
        "        plt.text(0.05, 0.95, f'Correlation: {corr:.3f}', \n",
        "                transform=plt.gca().transAxes, fontsize=12,\n",
        "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "        \n",
        "        if save_path:\n",
        "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "    \n",
        "    def visualize_predictions(self, images: np.ndarray, true_labels: Dict[str, np.ndarray], \n",
        "                            predictions: Dict[str, np.ndarray], model_name: str, \n",
        "                            num_samples: int = 10, save_path: str = None):\n",
        "        \"\"\"Visualize sample predictions\"\"\"\n",
        "        # Get correct and incorrect predictions\n",
        "        correct_mask = true_labels['expression'] == predictions['expression']\n",
        "        correct_indices = np.where(correct_mask)[0]\n",
        "        incorrect_indices = np.where(~correct_mask)[0]\n",
        "        \n",
        "        # Sample indices\n",
        "        correct_sample = np.random.choice(correct_indices, min(5, len(correct_indices)), replace=False)\n",
        "        incorrect_sample = np.random.choice(incorrect_indices, min(5, len(incorrect_indices)), replace=False)\n",
        "        \n",
        "        fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
        "        fig.suptitle(f'Prediction Samples - {model_name}', fontsize=16)\n",
        "        \n",
        "        # Plot correct predictions\n",
        "        for i, idx in enumerate(correct_sample):\n",
        "            if i < 5:\n",
        "                axes[0, i].imshow(images[idx])\n",
        "                axes[0, i].set_title(\n",
        "                    f'‚úì True: {self.config.CLASS_NAMES[true_labels[\"expression\"][idx]]}\\n'\n",
        "                    f'Pred: {self.config.CLASS_NAMES[predictions[\"expression\"][idx]]}\\n'\n",
        "                    f'V: {true_labels[\"valence\"][idx]:.2f}‚Üí{predictions[\"valence\"][idx]:.2f}\\n'\n",
        "                    f'A: {true_labels[\"arousal\"][idx]:.2f}‚Üí{predictions[\"arousal\"][idx]:.2f}',\n",
        "                    fontsize=10, color='green'\n",
        "                )\n",
        "                axes[0, i].axis('off')\n",
        "        \n",
        "        # Plot incorrect predictions\n",
        "        for i, idx in enumerate(incorrect_sample):\n",
        "            if i < 5:\n",
        "                axes[1, i].imshow(images[idx])\n",
        "                axes[1, i].set_title(\n",
        "                    f'‚úó True: {self.config.CLASS_NAMES[true_labels[\"expression\"][idx]]}\\n'\n",
        "                    f'Pred: {self.config.CLASS_NAMES[predictions[\"expression\"][idx]]}\\n'\n",
        "                    f'V: {true_labels[\"valence\"][idx]:.2f}‚Üí{predictions[\"valence\"][idx]:.2f}\\n'\n",
        "                    f'A: {true_labels[\"arousal\"][idx]:.2f}‚Üí{predictions[\"arousal\"][idx]:.2f}',\n",
        "                    fontsize=10, color='red'\n",
        "                )\n",
        "                axes[1, i].axis('off')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        if save_path:\n",
        "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Main Execution Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Dataset found!\n"
          ]
        }
      ],
      "source": [
        "# Initialize configuration and components\n",
        "config = Config()\n",
        "\n",
        "# Check if dataset exists\n",
        "if not os.path.exists(config.DATASET_PATH):\n",
        "    print(f\"‚ùå Dataset not found at {config.DATASET_PATH}\")\n",
        "    print(\"Please download and extract the dataset from the provided Google Drive link.\")\n",
        "else:\n",
        "    print(\"‚úÖ Dataset found!\")\n",
        "\n",
        "# Initialize components\n",
        "data_loader = FacialExpressionDataLoader(config)\n",
        "trainer = Trainer(config)\n",
        "evaluator = Evaluator(config)\n",
        "visualizer = Visualizer(config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Loading and preprocessing data...\n",
            "Loading dataset...\n",
            "Loaded 3999 valid samples from 3999 total images\n",
            "Training samples: 3199\n",
            "Validation samples: 800\n",
            "\n",
            "üìà Dataset Statistics:\n",
            "Total samples: 3999\n",
            "Training samples: 3199\n",
            "Validation/Test samples: 800\n",
            "Expression distribution: [500 500 500 500 500 500 500 499]\n",
            "Valence range: [-0.987, 0.982]\n",
            "Arousal range: [-0.667, 0.984]\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data\n",
        "print(\"\\nüìä Loading and preprocessing data...\")\n",
        "images, labels = data_loader.load_dataset()\n",
        "train_images, train_labels, val_images, val_labels = data_loader.create_data_splits(images, labels)\n",
        "\n",
        "# For testing, use validation set (in practice, you'd have a separate test set)\n",
        "test_images, test_labels = val_images, val_labels\n",
        "\n",
        "print(f\"\\nüìà Dataset Statistics:\")\n",
        "print(f\"Total samples: {len(images)}\")\n",
        "print(f\"Training samples: {len(train_images)}\")\n",
        "print(f\"Validation/Test samples: {len(val_images)}\")\n",
        "print(f\"Expression distribution: {np.bincount(labels['expression'])}\")\n",
        "print(f\"Valence range: [{labels['valence'].min():.3f}, {labels['valence'].max():.3f}]\")\n",
        "print(f\"Arousal range: [{labels['arousal'].min():.3f}, {labels['arousal'].max():.3f}]\")\n",
        "\n",
        "# Initialize data augmentation\n",
        "augmentation = DataAugmentation(config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üèóÔ∏è Building ResNet18 model...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "üéØ Training: ResNet18\n",
            "============================================================\n",
            "\n",
            "Training ResNet18...\n",
            "Model parameters: 11,181,642\n",
            "üîç Debug - Batch 1:\n",
            "  Images: min=-2.1179, max=2.6400, mean=-0.0068\n",
            "  Landmarks: min=-0.0428, max=1.1187, mean=0.5485\n",
            "  Expression targets: tensor([1, 6, 5, 5, 7], device='cuda:0')\n",
            "  Valence targets: tensor([ 0.5420, -0.3678, -0.7350, -0.8318, -0.6349], device='cuda:0')\n",
            "  Arousal targets: tensor([-0.5323,  0.6001,  0.4555,  0.3867,  0.6429], device='cuda:0')\n",
            "  Batch 20/200 (10.0%) - Loss: 2.0795\n",
            "  Batch 40/200 (20.0%) - Loss: 2.0950\n",
            "  Batch 60/200 (30.0%) - Loss: 2.0676\n",
            "  Batch 80/200 (40.0%) - Loss: 2.0403\n",
            "  Batch 100/200 (50.0%) - Loss: 2.0840\n",
            "  Batch 120/200 (60.0%) - Loss: 2.0382\n",
            "  Batch 140/200 (70.0%) - Loss: 2.0858\n",
            "  Batch 160/200 (80.0%) - Loss: 2.0118\n",
            "  Batch 180/200 (90.0%) - Loss: 2.0355\n",
            "  Batch 200/200 (100.0%) - Loss: 2.0881\n",
            "\n",
            "================================================================================\n",
            "EPOCH 1/10 - ResNet18\n",
            "================================================================================\n",
            "‚è±Ô∏è  Epoch Time: 824.18s\n",
            "üìä Training Metrics:\n",
            "   Loss: 2.0594 | Accuracy: 0.1922\n",
            "   Valence MSE: 0.2882 | Arousal MSE: 0.2374\n",
            "üìà Validation Metrics:\n",
            "   Loss: 2.0347 | Accuracy: 0.2387\n",
            "   Valence MSE: 0.2677 | Arousal MSE: 0.2195\n",
            "üéØ Learning Rate: 1.00e-05\n",
            "üîç Debug - Batch 1:\n",
            "  Images: min=-2.1179, max=2.5529, mean=-0.3992\n",
            "  Landmarks: min=-0.0494, max=1.0846, mean=0.5468\n",
            "  Expression targets: tensor([6, 7, 2, 4, 1], device='cuda:0')\n",
            "  Valence targets: tensor([-0.4603, -0.6146, -0.7160, -0.0238,  0.8130], device='cuda:0')\n",
            "  Arousal targets: tensor([ 0.6429,  0.6630, -0.1725,  0.7778,  0.2420], device='cuda:0')\n",
            "  Batch 20/200 (10.0%) - Loss: 2.0118\n",
            "  Batch 40/200 (20.0%) - Loss: 1.9992\n",
            "  Batch 60/200 (30.0%) - Loss: 2.0309\n",
            "  Batch 80/200 (40.0%) - Loss: 2.0336\n",
            "  Batch 100/200 (50.0%) - Loss: 2.0234\n",
            "  Batch 120/200 (60.0%) - Loss: 1.9599\n",
            "  Batch 140/200 (70.0%) - Loss: 1.9458\n",
            "  Batch 160/200 (80.0%) - Loss: 1.8731\n",
            "  Batch 180/200 (90.0%) - Loss: 1.9952\n",
            "  Batch 200/200 (100.0%) - Loss: 1.8704\n",
            "\n",
            "================================================================================\n",
            "EPOCH 2/10 - ResNet18\n",
            "================================================================================\n",
            "‚è±Ô∏è  Epoch Time: 1323.10s\n",
            "üìä Training Metrics:\n",
            "   Loss: 1.9896 | Accuracy: 0.2732\n",
            "   Valence MSE: 0.2864 | Arousal MSE: 0.2348\n",
            "üìà Validation Metrics:\n",
            "   Loss: 1.9536 | Accuracy: 0.2888\n",
            "   Valence MSE: 0.2664 | Arousal MSE: 0.2225\n",
            "üéØ Learning Rate: 9.78e-06\n",
            "üîç Debug - Batch 1:\n",
            "  Images: min=-2.1179, max=2.6400, mean=-0.2017\n",
            "  Landmarks: min=-0.0289, max=1.1500, mean=0.5438\n",
            "  Expression targets: tensor([2, 1, 5, 0, 4], device='cuda:0')\n",
            "  Valence targets: tensor([-0.9164,  0.5162, -0.8810,  0.0629, -0.0755], device='cuda:0')\n",
            "  Arousal targets: tensor([ 0.3759,  0.1106,  0.4127, -0.0048,  0.5713], device='cuda:0')\n",
            "  Batch 20/200 (10.0%) - Loss: 1.9158\n",
            "  Batch 40/200 (20.0%) - Loss: 2.0044\n",
            "  Batch 60/200 (30.0%) - Loss: 1.9598\n",
            "  Batch 80/200 (40.0%) - Loss: 1.8753\n",
            "  Batch 100/200 (50.0%) - Loss: 1.9630\n",
            "  Batch 120/200 (60.0%) - Loss: 1.8428\n",
            "  Batch 140/200 (70.0%) - Loss: 1.9181\n",
            "  Batch 160/200 (80.0%) - Loss: 1.8125\n",
            "  Batch 180/200 (90.0%) - Loss: 1.9299\n",
            "  Batch 200/200 (100.0%) - Loss: 1.8613\n",
            "\n",
            "================================================================================\n",
            "EPOCH 3/10 - ResNet18\n",
            "================================================================================\n",
            "‚è±Ô∏è  Epoch Time: 1087.61s\n",
            "üìä Training Metrics:\n",
            "   Loss: 1.8956 | Accuracy: 0.3273\n",
            "   Valence MSE: 0.2826 | Arousal MSE: 0.2364\n",
            "üìà Validation Metrics:\n",
            "   Loss: 1.8728 | Accuracy: 0.3275\n",
            "   Valence MSE: 0.2605 | Arousal MSE: 0.2269\n",
            "üéØ Learning Rate: 9.14e-06\n",
            "üîç Debug - Batch 1:\n",
            "  Images: min=-2.1179, max=2.6400, mean=0.2157\n",
            "  Landmarks: min=-0.0604, max=1.1738, mean=0.5468\n",
            "  Expression targets: tensor([7, 1, 2, 5, 1], device='cuda:0')\n",
            "  Valence targets: tensor([-0.5330,  0.9603, -0.7164, -0.3678,  0.8344], device='cuda:0')\n",
            "  Arousal targets: tensor([ 0.6774, -0.0714, -0.2265,  0.2371,  0.3227], device='cuda:0')\n",
            "  Batch 20/200 (10.0%) - Loss: 1.8629\n",
            "  Batch 40/200 (20.0%) - Loss: 1.8240\n",
            "  Batch 60/200 (30.0%) - Loss: 1.7746\n",
            "  Batch 80/200 (40.0%) - Loss: 1.6959\n",
            "  Batch 100/200 (50.0%) - Loss: 1.7048\n",
            "  Batch 120/200 (60.0%) - Loss: 1.7891\n",
            "  Batch 140/200 (70.0%) - Loss: 1.6496\n",
            "  Batch 160/200 (80.0%) - Loss: 1.8742\n",
            "  Batch 180/200 (90.0%) - Loss: 1.9953\n",
            "  Batch 200/200 (100.0%) - Loss: 1.6221\n",
            "\n",
            "================================================================================\n",
            "EPOCH 4/10 - ResNet18\n",
            "================================================================================\n",
            "‚è±Ô∏è  Epoch Time: 1089.29s\n",
            "üìä Training Metrics:\n",
            "   Loss: 1.8358 | Accuracy: 0.3570\n",
            "   Valence MSE: 0.2745 | Arousal MSE: 0.2343\n",
            "üìà Validation Metrics:\n",
            "   Loss: 1.8304 | Accuracy: 0.3450\n",
            "   Valence MSE: 0.2558 | Arousal MSE: 0.2258\n",
            "üéØ Learning Rate: 8.15e-06\n",
            "üîç Debug - Batch 1:\n",
            "  Images: min=-2.1179, max=2.6400, mean=-0.0210\n",
            "  Landmarks: min=-0.0515, max=1.1130, mean=0.5466\n",
            "  Expression targets: tensor([7, 4, 3, 5, 5], device='cuda:0')\n",
            "  Valence targets: tensor([-0.6001, -0.1508,  0.1208, -0.7160, -0.7564], device='cuda:0')\n",
            "  Arousal targets: tensor([0.6146, 0.6032, 0.4658, 0.4184, 0.4497], device='cuda:0')\n",
            "  Batch 20/200 (10.0%) - Loss: 1.6801\n",
            "  Batch 40/200 (20.0%) - Loss: 1.7438\n",
            "  Batch 60/200 (30.0%) - Loss: 1.9714\n",
            "  Batch 80/200 (40.0%) - Loss: 1.6946\n",
            "  Batch 100/200 (50.0%) - Loss: 1.7571\n",
            "  Batch 120/200 (60.0%) - Loss: 1.8971\n",
            "  Batch 140/200 (70.0%) - Loss: 1.7433\n",
            "  Batch 160/200 (80.0%) - Loss: 1.7510\n",
            "  Batch 180/200 (90.0%) - Loss: 1.8258\n",
            "  Batch 200/200 (100.0%) - Loss: 1.7743\n",
            "\n",
            "================================================================================\n",
            "EPOCH 5/10 - ResNet18\n",
            "================================================================================\n",
            "‚è±Ô∏è  Epoch Time: 1003.87s\n",
            "üìä Training Metrics:\n",
            "   Loss: 1.7927 | Accuracy: 0.3670\n",
            "   Valence MSE: 0.2751 | Arousal MSE: 0.2340\n",
            "üìà Validation Metrics:\n",
            "   Loss: 1.7959 | Accuracy: 0.3513\n",
            "   Valence MSE: 0.2573 | Arousal MSE: 0.2242\n",
            "üéØ Learning Rate: 6.89e-06\n",
            "üîç Debug - Batch 1:\n",
            "  Images: min=-2.1179, max=2.6400, mean=0.0307\n",
            "  Landmarks: min=-0.0224, max=1.2109, mean=0.5486\n",
            "  Expression targets: tensor([7, 1, 0, 0, 0], device='cuda:0')\n",
            "  Valence targets: tensor([-0.5874,  0.5579, -0.0324, -0.0532, -0.3146], device='cuda:0')\n",
            "  Arousal targets: tensor([ 0.6446,  0.2192, -0.0281, -0.1839, -0.1500], device='cuda:0')\n",
            "  Batch 20/200 (10.0%) - Loss: 1.6886\n",
            "  Batch 40/200 (20.0%) - Loss: 1.8260\n",
            "  Batch 60/200 (30.0%) - Loss: 1.7830\n",
            "  Batch 80/200 (40.0%) - Loss: 1.6474\n",
            "  Batch 100/200 (50.0%) - Loss: 1.9317\n",
            "  Batch 120/200 (60.0%) - Loss: 1.8873\n",
            "  Batch 140/200 (70.0%) - Loss: 1.7576\n",
            "  Batch 160/200 (80.0%) - Loss: 1.7161\n",
            "  Batch 180/200 (90.0%) - Loss: 1.9601\n",
            "  Batch 200/200 (100.0%) - Loss: 1.7320\n",
            "\n",
            "================================================================================\n",
            "EPOCH 6/10 - ResNet18\n",
            "================================================================================\n",
            "‚è±Ô∏è  Epoch Time: 1085.56s\n",
            "üìä Training Metrics:\n",
            "   Loss: 1.7567 | Accuracy: 0.3920\n",
            "   Valence MSE: 0.2726 | Arousal MSE: 0.2299\n",
            "üìà Validation Metrics:\n",
            "   Loss: 1.7744 | Accuracy: 0.3775\n",
            "   Valence MSE: 0.2548 | Arousal MSE: 0.2258\n",
            "üéØ Learning Rate: 5.50e-06\n",
            "üîç Debug - Batch 1:\n",
            "  Images: min=-2.1179, max=2.6400, mean=-0.1444\n",
            "  Landmarks: min=-0.0491, max=1.2709, mean=0.5538\n",
            "  Expression targets: tensor([4, 5, 0, 0, 5], device='cuda:0')\n",
            "  Valence targets: tensor([-0.1111, -0.7376, -0.0194, -0.2070, -0.5317], device='cuda:0')\n",
            "  Arousal targets: tensor([ 0.4365,  0.4098, -0.0194, -0.0906,  0.2063], device='cuda:0')\n",
            "  Batch 20/200 (10.0%) - Loss: 1.7404\n",
            "  Batch 40/200 (20.0%) - Loss: 1.5437\n",
            "  Batch 60/200 (30.0%) - Loss: 1.6921\n",
            "  Batch 80/200 (40.0%) - Loss: 1.7188\n",
            "  Batch 100/200 (50.0%) - Loss: 1.6669\n",
            "  Batch 120/200 (60.0%) - Loss: 1.7576\n",
            "  Batch 140/200 (70.0%) - Loss: 1.8220\n",
            "  Batch 160/200 (80.0%) - Loss: 1.7319\n",
            "  Batch 180/200 (90.0%) - Loss: 1.5509\n",
            "  Batch 200/200 (100.0%) - Loss: 1.6683\n",
            "\n",
            "================================================================================\n",
            "EPOCH 7/10 - ResNet18\n",
            "================================================================================\n",
            "‚è±Ô∏è  Epoch Time: 1094.50s\n",
            "üìä Training Metrics:\n",
            "   Loss: 1.7274 | Accuracy: 0.4073\n",
            "   Valence MSE: 0.2741 | Arousal MSE: 0.2323\n",
            "üìà Validation Metrics:\n",
            "   Loss: 1.7599 | Accuracy: 0.3837\n",
            "   Valence MSE: 0.2575 | Arousal MSE: 0.2321\n",
            "üéØ Learning Rate: 4.11e-06\n",
            "üîç Debug - Batch 1:\n",
            "  Images: min=-2.1179, max=2.6400, mean=0.0690\n",
            "  Landmarks: min=-0.0962, max=1.2444, mean=0.5497\n",
            "  Expression targets: tensor([4, 0, 6, 5, 3], device='cuda:0')\n",
            "  Valence targets: tensor([-0.1096, -0.0556, -0.2904, -0.8968, -0.0948], device='cuda:0')\n",
            "  Arousal targets: tensor([ 0.8318, -0.0556,  0.4743,  0.3810,  0.8428], device='cuda:0')\n",
            "  Batch 20/200 (10.0%) - Loss: 1.6955\n",
            "  Batch 40/200 (20.0%) - Loss: 1.7743\n",
            "  Batch 60/200 (30.0%) - Loss: 1.9511\n",
            "  Batch 80/200 (40.0%) - Loss: 1.7703\n",
            "  Batch 100/200 (50.0%) - Loss: 1.5559\n",
            "  Batch 120/200 (60.0%) - Loss: 1.8261\n",
            "  Batch 140/200 (70.0%) - Loss: 1.7745\n",
            "  Batch 160/200 (80.0%) - Loss: 1.9125\n",
            "  Batch 180/200 (90.0%) - Loss: 1.7960\n",
            "  Batch 200/200 (100.0%) - Loss: 1.6400\n",
            "\n",
            "================================================================================\n",
            "EPOCH 8/10 - ResNet18\n",
            "================================================================================\n",
            "‚è±Ô∏è  Epoch Time: 1051.62s\n",
            "üìä Training Metrics:\n",
            "   Loss: 1.7174 | Accuracy: 0.4126\n",
            "   Valence MSE: 0.2745 | Arousal MSE: 0.2298\n",
            "üìà Validation Metrics:\n",
            "   Loss: 1.7460 | Accuracy: 0.3912\n",
            "   Valence MSE: 0.2574 | Arousal MSE: 0.2246\n",
            "üéØ Learning Rate: 2.85e-06\n",
            "üîç Debug - Batch 1:\n",
            "  Images: min=-2.1179, max=2.6400, mean=0.3356\n",
            "  Landmarks: min=-0.0536, max=1.0753, mean=0.5457\n",
            "  Expression targets: tensor([7, 1, 4, 7, 4], device='cuda:0')\n",
            "  Valence targets: tensor([-0.6746,  0.7840, -0.0728, -0.6485, -0.1528], device='cuda:0')\n",
            "  Arousal targets: tensor([0.6429, 0.0823, 0.7870, 0.6436, 0.7743], device='cuda:0')\n",
            "  Batch 20/200 (10.0%) - Loss: 1.6368\n",
            "  Batch 40/200 (20.0%) - Loss: 1.5809\n",
            "  Batch 60/200 (30.0%) - Loss: 1.6585\n",
            "  Batch 80/200 (40.0%) - Loss: 1.8394\n",
            "  Batch 100/200 (50.0%) - Loss: 1.6311\n",
            "  Batch 120/200 (60.0%) - Loss: 1.8289\n",
            "  Batch 140/200 (70.0%) - Loss: 1.6398\n",
            "  Batch 160/200 (80.0%) - Loss: 1.6375\n",
            "  Batch 180/200 (90.0%) - Loss: 1.8363\n",
            "  Batch 200/200 (100.0%) - Loss: 1.5500\n",
            "\n",
            "================================================================================\n",
            "EPOCH 9/10 - ResNet18\n",
            "================================================================================\n",
            "‚è±Ô∏è  Epoch Time: 1307.50s\n",
            "üìä Training Metrics:\n",
            "   Loss: 1.7023 | Accuracy: 0.4201\n",
            "   Valence MSE: 0.2724 | Arousal MSE: 0.2302\n",
            "üìà Validation Metrics:\n",
            "   Loss: 1.7411 | Accuracy: 0.3875\n",
            "   Valence MSE: 0.2577 | Arousal MSE: 0.2339\n",
            "üéØ Learning Rate: 1.86e-06\n",
            "üîç Debug - Batch 1:\n",
            "  Images: min=-2.1179, max=2.6400, mean=0.1888\n",
            "  Landmarks: min=-0.0410, max=1.1584, mean=0.5405\n",
            "  Expression targets: tensor([3, 0, 3, 1, 1], device='cuda:0')\n",
            "  Valence targets: tensor([0.3194, 0.0948, 0.0000, 0.9206, 0.3629], device='cuda:0')\n",
            "  Arousal targets: tensor([ 0.7743, -0.4214,  0.7619,  0.2698, -0.0097], device='cuda:0')\n",
            "  Batch 20/200 (10.0%) - Loss: 1.7005\n",
            "  Batch 40/200 (20.0%) - Loss: 1.7431\n",
            "  Batch 60/200 (30.0%) - Loss: 1.5640\n",
            "  Batch 80/200 (40.0%) - Loss: 1.7981\n",
            "  Batch 100/200 (50.0%) - Loss: 1.9457\n",
            "  Batch 120/200 (60.0%) - Loss: 1.6535\n",
            "  Batch 140/200 (70.0%) - Loss: 1.6987\n",
            "  Batch 160/200 (80.0%) - Loss: 1.6691\n",
            "  Batch 180/200 (90.0%) - Loss: 1.7144\n",
            "  Batch 200/200 (100.0%) - Loss: 1.7875\n",
            "\n",
            "================================================================================\n",
            "EPOCH 10/10 - ResNet18\n",
            "================================================================================\n",
            "‚è±Ô∏è  Epoch Time: 1402.75s\n",
            "üìä Training Metrics:\n",
            "   Loss: 1.7078 | Accuracy: 0.4161\n",
            "   Valence MSE: 0.2747 | Arousal MSE: 0.2300\n",
            "üìà Validation Metrics:\n",
            "   Loss: 1.7374 | Accuracy: 0.3950\n",
            "   Valence MSE: 0.2563 | Arousal MSE: 0.2251\n",
            "üéØ Learning Rate: 1.22e-06\n",
            "\n",
            "ResNet18 training completed in 11271.25 seconds\n"
          ]
        }
      ],
      "source": [
        "# Build and train ResNet18 model\n",
        "print(\"\\nüèóÔ∏è Building ResNet18 model...\")\n",
        "model_resnet = build_resnet18_multitask(config.NUM_CLASSES)\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"üéØ Training: ResNet18\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "training_result_resnet = trainer.train_model(\n",
        "    model_resnet,\n",
        "    (train_images, train_labels),\n",
        "    (val_images, val_labels),\n",
        "    'ResNet18',\n",
        "    augmentation\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üèóÔ∏è Building MobileNetV2 model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to C:\\Users\\Admin/.cache\\torch\\hub\\checkpoints\\mobilenet_v2-b0353104.pth\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13.6M/13.6M [00:04<00:00, 3.25MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "üéØ Training: MobileNetV2\n",
            "============================================================\n",
            "\n",
            "Training MobileNetV2...\n",
            "Model parameters: 2,236,682\n",
            "üîç Debug - Batch 1:\n",
            "  Images: min=-2.1179, max=2.6400, mean=0.0524\n",
            "  Landmarks: min=-0.0237, max=1.0853, mean=0.5413\n",
            "  Expression targets: tensor([0, 2, 6, 3, 7], device='cuda:0')\n",
            "  Valence targets: tensor([ 0.0629, -0.7549, -0.7480,  0.4444, -0.6388], device='cuda:0')\n",
            "  Arousal targets: tensor([-0.0151, -0.3388,  0.4477,  0.8730,  0.6436], device='cuda:0')\n",
            "  Batch 20/200 (10.0%) - Loss: 2.0603\n",
            "  Batch 40/200 (20.0%) - Loss: 2.0845\n",
            "  Batch 60/200 (30.0%) - Loss: 2.0953\n",
            "  Batch 80/200 (40.0%) - Loss: 2.0646\n",
            "  Batch 100/200 (50.0%) - Loss: 2.0716\n",
            "  Batch 120/200 (60.0%) - Loss: 2.0737\n",
            "  Batch 140/200 (70.0%) - Loss: 2.0760\n",
            "  Batch 160/200 (80.0%) - Loss: 2.0512\n",
            "  Batch 180/200 (90.0%) - Loss: 2.0578\n",
            "  Batch 200/200 (100.0%) - Loss: 2.0269\n",
            "\n",
            "================================================================================\n",
            "EPOCH 1/10 - MobileNetV2\n",
            "================================================================================\n",
            "‚è±Ô∏è  Epoch Time: 2541.90s\n",
            "üìä Training Metrics:\n",
            "   Loss: 2.0688 | Accuracy: 0.1550\n",
            "   Valence MSE: 0.2456 | Arousal MSE: 0.1705\n",
            "üìà Validation Metrics:\n",
            "   Loss: 2.0550 | Accuracy: 0.2162\n",
            "   Valence MSE: 0.2221 | Arousal MSE: 0.1816\n",
            "üéØ Learning Rate: 1.00e-05\n",
            "üîç Debug - Batch 1:\n",
            "  Images: min=-2.1179, max=2.6400, mean=0.0268\n",
            "  Landmarks: min=-0.1012, max=1.1695, mean=0.5488\n",
            "  Expression targets: tensor([1, 7, 5, 7, 5], device='cuda:0')\n",
            "  Valence targets: tensor([ 0.8230, -0.2222, -0.9146, -0.2565, -0.8016], device='cuda:0')\n",
            "  Arousal targets: tensor([0.1531, 0.5873, 0.2855, 0.2855, 0.2917], device='cuda:0')\n",
            "  Batch 20/200 (10.0%) - Loss: 2.0372\n",
            "  Batch 40/200 (20.0%) - Loss: 2.0597\n",
            "  Batch 60/200 (30.0%) - Loss: 1.9882\n",
            "  Batch 80/200 (40.0%) - Loss: 2.0617\n",
            "  Batch 100/200 (50.0%) - Loss: 2.0260\n",
            "  Batch 120/200 (60.0%) - Loss: 1.9779\n",
            "  Batch 140/200 (70.0%) - Loss: 2.0261\n",
            "  Batch 160/200 (80.0%) - Loss: 1.9549\n",
            "  Batch 180/200 (90.0%) - Loss: 2.0012\n",
            "  Batch 200/200 (100.0%) - Loss: 2.0273\n",
            "\n",
            "================================================================================\n",
            "EPOCH 2/10 - MobileNetV2\n",
            "================================================================================\n",
            "‚è±Ô∏è  Epoch Time: 1712.59s\n",
            "üìä Training Metrics:\n",
            "   Loss: 2.0209 | Accuracy: 0.2551\n",
            "   Valence MSE: 0.2482 | Arousal MSE: 0.1689\n",
            "üìà Validation Metrics:\n",
            "   Loss: 1.9979 | Accuracy: 0.2412\n",
            "   Valence MSE: 0.2228 | Arousal MSE: 0.1831\n",
            "üéØ Learning Rate: 9.78e-06\n",
            "üîç Debug - Batch 1:\n",
            "  Images: min=-2.1179, max=2.6400, mean=0.1203\n",
            "  Landmarks: min=-0.0619, max=1.1216, mean=0.5507\n",
            "  Expression targets: tensor([6, 7, 0, 6, 3], device='cuda:0')\n",
            "  Valence targets: tensor([-0.7585, -0.6243, -0.0043, -0.4364,  0.4307], device='cuda:0')\n",
            "  Arousal targets: tensor([ 0.4530,  0.6630, -0.0173,  0.7082,  0.8420], device='cuda:0')\n",
            "  Batch 20/200 (10.0%) - Loss: 1.9760\n",
            "  Batch 40/200 (20.0%) - Loss: 1.9717\n",
            "  Batch 60/200 (30.0%) - Loss: 1.9500\n",
            "  Batch 80/200 (40.0%) - Loss: 1.9455\n",
            "  Batch 100/200 (50.0%) - Loss: 2.0670\n",
            "  Batch 120/200 (60.0%) - Loss: 2.0176\n",
            "  Batch 140/200 (70.0%) - Loss: 1.9079\n",
            "  Batch 160/200 (80.0%) - Loss: 1.9673\n",
            "  Batch 180/200 (90.0%) - Loss: 1.9630\n",
            "  Batch 200/200 (100.0%) - Loss: 1.8458\n",
            "\n",
            "================================================================================\n",
            "EPOCH 3/10 - MobileNetV2\n",
            "================================================================================\n",
            "‚è±Ô∏è  Epoch Time: 1640.08s\n",
            "üìä Training Metrics:\n",
            "   Loss: 1.9599 | Accuracy: 0.2851\n",
            "   Valence MSE: 0.2464 | Arousal MSE: 0.1704\n",
            "üìà Validation Metrics:\n",
            "   Loss: 1.9398 | Accuracy: 0.2750\n",
            "   Valence MSE: 0.2220 | Arousal MSE: 0.1816\n",
            "üéØ Learning Rate: 9.14e-06\n",
            "üîç Debug - Batch 1:\n",
            "  Images: min=-2.1179, max=2.6400, mean=0.0722\n",
            "  Landmarks: min=-0.0747, max=1.0926, mean=0.5476\n",
            "  Expression targets: tensor([0, 0, 6, 1, 1], device='cuda:0')\n",
            "  Valence targets: tensor([-0.0043,  0.0079, -0.4365,  0.7962,  0.7791], device='cuda:0')\n",
            "  Arousal targets: tensor([-0.0108,  0.0000,  0.8492,  0.1837,  0.1016], device='cuda:0')\n",
            "  Batch 20/200 (10.0%) - Loss: 1.9208\n",
            "  Batch 40/200 (20.0%) - Loss: 1.8652\n",
            "  Batch 60/200 (30.0%) - Loss: 1.8760\n",
            "  Batch 80/200 (40.0%) - Loss: 1.9077\n",
            "  Batch 100/200 (50.0%) - Loss: 2.0103\n",
            "  Batch 120/200 (60.0%) - Loss: 1.7735\n",
            "  Batch 140/200 (70.0%) - Loss: 1.8688\n",
            "  Batch 160/200 (80.0%) - Loss: 1.9799\n",
            "  Batch 180/200 (90.0%) - Loss: 2.0843\n",
            "  Batch 200/200 (100.0%) - Loss: 2.0575\n",
            "\n",
            "================================================================================\n",
            "EPOCH 4/10 - MobileNetV2\n",
            "================================================================================\n",
            "‚è±Ô∏è  Epoch Time: 1881.23s\n",
            "üìä Training Metrics:\n",
            "   Loss: 1.9045 | Accuracy: 0.3001\n",
            "   Valence MSE: 0.2475 | Arousal MSE: 0.1711\n",
            "üìà Validation Metrics:\n",
            "   Loss: 1.8998 | Accuracy: 0.2963\n",
            "   Valence MSE: 0.2245 | Arousal MSE: 0.1848\n",
            "üéØ Learning Rate: 8.15e-06\n",
            "üîç Debug - Batch 1:\n",
            "  Images: min=-2.1179, max=2.6400, mean=-0.0864\n",
            "  Landmarks: min=-0.1619, max=1.1725, mean=0.5459\n",
            "  Expression targets: tensor([1, 2, 1, 4, 7], device='cuda:0')\n",
            "  Valence targets: tensor([ 0.6283, -0.5642,  0.6825, -0.1258, -0.6190], device='cuda:0')\n",
            "  Arousal targets: tensor([ 0.0728, -0.3934,  0.2063,  0.7453,  0.6270], device='cuda:0')\n",
            "  Batch 20/200 (10.0%) - Loss: 1.8549\n",
            "  Batch 40/200 (20.0%) - Loss: 1.9152\n",
            "  Batch 60/200 (30.0%) - Loss: 1.9409\n",
            "  Batch 80/200 (40.0%) - Loss: 1.8818\n",
            "  Batch 100/200 (50.0%) - Loss: 1.9302\n",
            "  Batch 120/200 (60.0%) - Loss: 1.8398\n",
            "  Batch 140/200 (70.0%) - Loss: 1.7388\n",
            "  Batch 160/200 (80.0%) - Loss: 1.8672\n",
            "  Batch 180/200 (90.0%) - Loss: 1.8114\n",
            "  Batch 200/200 (100.0%) - Loss: 1.9263\n",
            "\n",
            "================================================================================\n",
            "EPOCH 5/10 - MobileNetV2\n",
            "================================================================================\n",
            "‚è±Ô∏è  Epoch Time: 1662.88s\n",
            "üìä Training Metrics:\n",
            "   Loss: 1.8603 | Accuracy: 0.3260\n",
            "   Valence MSE: 0.2487 | Arousal MSE: 0.1722\n",
            "üìà Validation Metrics:\n",
            "   Loss: 1.8675 | Accuracy: 0.2963\n",
            "   Valence MSE: 0.2254 | Arousal MSE: 0.1885\n",
            "üéØ Learning Rate: 6.89e-06\n",
            "üîç Debug - Batch 1:\n",
            "  Images: min=-2.1179, max=2.6400, mean=-0.0714\n",
            "  Landmarks: min=-0.1995, max=1.1960, mean=0.5530\n",
            "  Expression targets: tensor([1, 6, 5, 4, 7], device='cuda:0')\n",
            "  Valence targets: tensor([ 0.8227, -0.4524, -0.8492, -0.1791, -0.6001], device='cuda:0')\n",
            "  Arousal targets: tensor([0.1307, 0.7381, 0.4048, 0.7695, 0.6581], device='cuda:0')\n",
            "  Batch 20/200 (10.0%) - Loss: 1.9053\n",
            "  Batch 40/200 (20.0%) - Loss: 1.9053\n",
            "  Batch 60/200 (30.0%) - Loss: 1.8757\n",
            "  Batch 80/200 (40.0%) - Loss: 1.8982\n",
            "  Batch 100/200 (50.0%) - Loss: 2.0488\n",
            "  Batch 120/200 (60.0%) - Loss: 1.8296\n",
            "  Batch 140/200 (70.0%) - Loss: 1.7490\n",
            "  Batch 160/200 (80.0%) - Loss: 1.9494\n",
            "  Batch 180/200 (90.0%) - Loss: 1.8674\n",
            "  Batch 200/200 (100.0%) - Loss: 1.8748\n",
            "\n",
            "================================================================================\n",
            "EPOCH 6/10 - MobileNetV2\n",
            "================================================================================\n",
            "‚è±Ô∏è  Epoch Time: 2196.47s\n",
            "üìä Training Metrics:\n",
            "   Loss: 1.8320 | Accuracy: 0.3342\n",
            "   Valence MSE: 0.2526 | Arousal MSE: 0.1726\n",
            "üìà Validation Metrics:\n",
            "   Loss: 1.8543 | Accuracy: 0.3038\n",
            "   Valence MSE: 0.2275 | Arousal MSE: 0.1872\n",
            "üéØ Learning Rate: 5.50e-06\n",
            "üîç Debug - Batch 1:\n",
            "  Images: min=-2.1179, max=2.6400, mean=-0.2888\n",
            "  Landmarks: min=-0.0971, max=1.1067, mean=0.5425\n",
            "  Expression targets: tensor([2, 0, 6, 0, 3], device='cuda:0')\n",
            "  Valence targets: tensor([-0.6969,  0.0000, -0.4732,  0.0129,  0.3097], device='cuda:0')\n",
            "  Arousal targets: tensor([-0.2662,  0.0159,  0.8069, -0.0129,  0.7356], device='cuda:0')\n",
            "  Batch 20/200 (10.0%) - Loss: 1.8484\n",
            "  Batch 40/200 (20.0%) - Loss: 1.9141\n",
            "  Batch 60/200 (30.0%) - Loss: 1.7981\n",
            "  Batch 80/200 (40.0%) - Loss: 1.7888\n",
            "  Batch 100/200 (50.0%) - Loss: 2.0410\n",
            "  Batch 120/200 (60.0%) - Loss: 1.7697\n",
            "  Batch 140/200 (70.0%) - Loss: 1.7870\n",
            "  Batch 160/200 (80.0%) - Loss: 1.8515\n",
            "  Batch 180/200 (90.0%) - Loss: 1.9028\n",
            "  Batch 200/200 (100.0%) - Loss: 1.8243\n",
            "\n",
            "================================================================================\n",
            "EPOCH 7/10 - MobileNetV2\n",
            "================================================================================\n",
            "‚è±Ô∏è  Epoch Time: 2162.23s\n",
            "üìä Training Metrics:\n",
            "   Loss: 1.8101 | Accuracy: 0.3460\n",
            "   Valence MSE: 0.2546 | Arousal MSE: 0.1740\n",
            "üìà Validation Metrics:\n",
            "   Loss: 1.8284 | Accuracy: 0.3287\n",
            "   Valence MSE: 0.2291 | Arousal MSE: 0.1875\n",
            "üéØ Learning Rate: 4.11e-06\n",
            "üîç Debug - Batch 1:\n",
            "  Images: min=-2.1179, max=2.6400, mean=-0.1652\n",
            "  Landmarks: min=-0.0438, max=1.1135, mean=0.5455\n",
            "  Expression targets: tensor([7, 7, 6, 0, 4], device='cuda:0')\n",
            "  Valence targets: tensor([-0.5904, -0.5421, -0.5710, -0.4214, -0.1158], device='cuda:0')\n",
            "  Arousal targets: tensor([0.7065, 0.6378, 0.7259, 0.1738, 0.8633], device='cuda:0')\n",
            "  Batch 20/200 (10.0%) - Loss: 1.7653\n",
            "  Batch 40/200 (20.0%) - Loss: 2.0193\n",
            "  Batch 60/200 (30.0%) - Loss: 1.5812\n",
            "  Batch 80/200 (40.0%) - Loss: 1.8350\n",
            "  Batch 100/200 (50.0%) - Loss: 1.5408\n",
            "  Batch 120/200 (60.0%) - Loss: 1.9565\n",
            "  Batch 140/200 (70.0%) - Loss: 1.7538\n",
            "  Batch 160/200 (80.0%) - Loss: 1.7755\n",
            "  Batch 180/200 (90.0%) - Loss: 1.6157\n",
            "  Batch 200/200 (100.0%) - Loss: 1.7901\n",
            "\n",
            "================================================================================\n",
            "EPOCH 8/10 - MobileNetV2\n",
            "================================================================================\n",
            "‚è±Ô∏è  Epoch Time: 2225.39s\n",
            "üìä Training Metrics:\n",
            "   Loss: 1.8034 | Accuracy: 0.3517\n",
            "   Valence MSE: 0.2539 | Arousal MSE: 0.1736\n",
            "üìà Validation Metrics:\n",
            "   Loss: 1.8184 | Accuracy: 0.3325\n",
            "   Valence MSE: 0.2283 | Arousal MSE: 0.1865\n",
            "üéØ Learning Rate: 2.85e-06\n",
            "üîç Debug - Batch 1:\n",
            "  Images: min=-2.1179, max=2.6400, mean=0.1360\n",
            "  Landmarks: min=-0.0198, max=1.1216, mean=0.5460\n",
            "  Expression targets: tensor([6, 7, 0, 3, 4], device='cuda:0')\n",
            "  Valence targets: tensor([-0.1508, -0.6321, -0.1746,  0.3636, -0.0871], device='cuda:0')\n",
            "  Arousal targets: tensor([0.3492, 0.6532, 0.2222, 0.6127, 0.5952], device='cuda:0')\n",
            "  Batch 20/200 (10.0%) - Loss: 1.8367\n",
            "  Batch 40/200 (20.0%) - Loss: 1.8865\n",
            "  Batch 60/200 (30.0%) - Loss: 1.7551\n",
            "  Batch 80/200 (40.0%) - Loss: 1.6189\n",
            "  Batch 100/200 (50.0%) - Loss: 1.9717\n",
            "  Batch 120/200 (60.0%) - Loss: 1.9058\n",
            "  Batch 140/200 (70.0%) - Loss: 1.7720\n",
            "  Batch 160/200 (80.0%) - Loss: 1.7113\n",
            "  Batch 180/200 (90.0%) - Loss: 1.8057\n",
            "  Batch 200/200 (100.0%) - Loss: 1.8472\n",
            "\n",
            "================================================================================\n",
            "EPOCH 9/10 - MobileNetV2\n",
            "================================================================================\n",
            "‚è±Ô∏è  Epoch Time: 2134.49s\n",
            "üìä Training Metrics:\n",
            "   Loss: 1.7820 | Accuracy: 0.3654\n",
            "   Valence MSE: 0.2564 | Arousal MSE: 0.1745\n",
            "üìà Validation Metrics:\n",
            "   Loss: 1.8187 | Accuracy: 0.3200\n",
            "   Valence MSE: 0.2303 | Arousal MSE: 0.1875\n",
            "üéØ Learning Rate: 1.86e-06\n",
            "üîç Debug - Batch 1:\n",
            "  Images: min=-2.1179, max=2.6400, mean=-0.2705\n",
            "  Landmarks: min=-0.0771, max=1.0981, mean=0.5427\n",
            "  Expression targets: tensor([6, 2, 4, 2, 4], device='cuda:0')\n",
            "  Valence targets: tensor([-0.4632, -0.4524, -0.1146, -0.8418, -0.1016], device='cuda:0')\n",
            "  Arousal targets: tensor([ 0.8219, -0.1905,  0.8567, -0.1594,  0.6291], device='cuda:0')\n",
            "  Batch 20/200 (10.0%) - Loss: 1.8124\n",
            "  Batch 40/200 (20.0%) - Loss: 1.7843\n",
            "  Batch 60/200 (30.0%) - Loss: 1.7657\n",
            "  Batch 80/200 (40.0%) - Loss: 1.6275\n",
            "  Batch 100/200 (50.0%) - Loss: 1.8351\n",
            "  Batch 120/200 (60.0%) - Loss: 1.7270\n",
            "  Batch 140/200 (70.0%) - Loss: 1.7215\n",
            "  Batch 160/200 (80.0%) - Loss: 1.6966\n",
            "  Batch 180/200 (90.0%) - Loss: 1.8376\n",
            "  Batch 200/200 (100.0%) - Loss: 1.8123\n",
            "\n",
            "================================================================================\n",
            "EPOCH 10/10 - MobileNetV2\n",
            "================================================================================\n",
            "‚è±Ô∏è  Epoch Time: 2168.97s\n",
            "üìä Training Metrics:\n",
            "   Loss: 1.7774 | Accuracy: 0.3676\n",
            "   Valence MSE: 0.2555 | Arousal MSE: 0.1742\n",
            "üìà Validation Metrics:\n",
            "   Loss: 1.8124 | Accuracy: 0.3325\n",
            "   Valence MSE: 0.2279 | Arousal MSE: 0.1861\n",
            "üéØ Learning Rate: 1.22e-06\n",
            "\n",
            "MobileNetV2 training completed in 20326.84 seconds\n"
          ]
        }
      ],
      "source": [
        "# Build and train MobileNetV2 model\n",
        "print(\"\\nüèóÔ∏è Building MobileNetV2 model...\")\n",
        "model_mobilenet = build_mobilenet_v2_multitask(config.NUM_CLASSES)\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"üéØ Training: MobileNetV2\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "training_result_mobilenet = trainer.train_model(\n",
        "    model_mobilenet,\n",
        "    (train_images, train_labels),\n",
        "    (val_images, val_labels),\n",
        "    'MobileNetV2',\n",
        "    augmentation\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Evaluating: ResNet18\n",
            "\n",
            "Evaluating ResNet18...\n",
            "\n",
            "ResNet18 Results:\n",
            "Expression Accuracy: 0.3950\n",
            "Expression F1 (macro): 0.3863\n",
            "Valence RMSE: 0.5062\n",
            "Valence CCC: 0.0249\n",
            "Arousal RMSE: 0.4744\n",
            "Arousal CCC: 0.0084\n",
            "\n",
            "üìä Evaluating: MobileNetV2\n",
            "\n",
            "Evaluating MobileNetV2...\n",
            "\n",
            "MobileNetV2 Results:\n",
            "Expression Accuracy: 0.3287\n",
            "Expression F1 (macro): 0.2988\n",
            "Valence RMSE: 0.4773\n",
            "Valence CCC: -0.0410\n",
            "Arousal RMSE: 0.4313\n",
            "Arousal CCC: -0.0314\n"
          ]
        }
      ],
      "source": [
        "# Evaluate both models\n",
        "print(f\"\\nüìä Evaluating: ResNet18\")\n",
        "evaluation_result_resnet = evaluator.evaluate_model(\n",
        "    training_result_resnet['model'],\n",
        "    (test_images, test_labels),\n",
        "    'ResNet18'\n",
        ")\n",
        "\n",
        "print(f\"\\nüìä Evaluating: MobileNetV2\")\n",
        "evaluation_result_mobilenet = evaluator.evaluate_model(\n",
        "    training_result_mobilenet['model'],\n",
        "    (test_images, test_labels),\n",
        "    'MobileNetV2'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Model Comparison Results:\n",
            "      Model  Expression_Accuracy  Expression_F1  Expression_Kappa  Valence_RMSE  Valence_CCC  Valence_Corr  Arousal_RMSE  Arousal_CCC  Arousal_Corr\n",
            "   ResNet18              0.39500       0.386350          0.308773      0.506227      0.02486      0.082454      0.474414     0.008391      0.025112\n",
            "MobileNetV2              0.32875       0.298833          0.233087      0.477285     -0.04098     -0.129536      0.431317    -0.031362     -0.105461\n"
          ]
        }
      ],
      "source": [
        "# Create comparison table\n",
        "print(\"\\nüìä Model Comparison Results:\")\n",
        "comparison_table = evaluator.create_comparison_table()\n",
        "print(comparison_table.to_string(index=False))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
